{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO3mb7YLw6sm/X908rGoqqk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ufbfung/pubmed/blob/main/pubmed_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        "This notebook will be used to develop a lightweight, literature assistant that helps with answering research questions by compiling a list of articles from pubmed based on the search keywords provided.\n",
        "\n",
        "## OpenAI Error codes\n",
        "|Status code|Error Type|\n",
        "|--|--|\n",
        "|400|BadRequestError|\n",
        "|401|AuthenticationError|\n",
        "|403|PermissionDeniedError|"
      ],
      "metadata": {
        "id": "50vUpJG0bJZo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Development"
      ],
      "metadata": {
        "id": "aiKdvpYwb1su"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install beautifulsoup4\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "ivsExIwjXUaD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11dac7ca-8796-4024-dcef-3724e3ec53d0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.3.7)\n",
            "Requirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.11.17)\n",
            "Installing collected packages: tiktoken\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tiktoken-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "lgNAFY18VVrA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df055cd4-6e95-41b7-984a-7f642306b5a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a question: will ai replace radiologists?\n",
            "Enter a list of terms separated by spaces: ai radiologists\n",
            "['ai', 'radiologists']\n",
            "Answer:\n",
            "\n",
            "\n",
            "Based on the retrieved abstracts, I think the answer to your question is yes. This is supported by the implementation of an AI software device (qXR) for TB screening using CXR images, which was able to identify additional cases that were not deemed presumptive for TB by radiologists. This highlights the potential of AI in improving the accuracy of TB screening and the need for seamless deployment and integration in resource-limited settings. Additionally, a separate study also found that deep learning-based AI systems show promising performance in assisting radiologists in breast cancer screening.\n",
            "\n",
            "Table of Retrieved Articles:\n",
            "PubMed ID\tTitle\t\t\t\t\t\t\t\t\t\t\t\t\t\tURL\n",
            "38060461\t\tImplementing a chest X-ray artificial intelligence tool to enhance tuberculosis screening in India: Lessons learned\t\t\t\thttps://pubmed.ncbi.nlm.nih.gov/38060461/\n",
            "38043630\t\tEvaluation of Climate-Aware Metrics Tools for Radiology Informatics and Artificial Intelligence: Towards a Potential Radiology Eco-Label\t\t\t\thttps://pubmed.ncbi.nlm.nih.gov/38043630/\n",
            "38026220\t\tBeyond radiologist-level liver lesion detection on multi-phase contrast-enhanced CT images by deep learning\t\t\t\thttps://pubmed.ncbi.nlm.nih.gov/38026220/\n",
            "38023149\t\tA classifier model for prostate cancer diagnosis using CNNs and transfer learning with multi-parametric MRI\t\t\t\thttps://pubmed.ncbi.nlm.nih.gov/38023149/\n",
            "38011214\t\tAn evidence-based approach to artificial intelligence education for medical students: A systematic review\t\t\t\thttps://pubmed.ncbi.nlm.nih.gov/38011214/\n",
            "38006614\t\tLinguistic precision, and declared use of ChatGPT, needed for radiology literature\t\t\t\thttps://pubmed.ncbi.nlm.nih.gov/38006614/\n",
            "37998607\t\tEvaluation of a Decision Support System Developed with Deep Learning Approach for Detecting Dental Caries with Cone-Beam Computed Tomography Imaging\t\t\t\thttps://pubmed.ncbi.nlm.nih.gov/37998607/\n",
            "37998543\t\tUsing Artificial Intelligence to Stratify Normal versus Abnormal Chest X-rays: External Validation of a Deep Learning Algorithm at East Kent Hospitals University NHS Foundation Trust\t\t\t\thttps://pubmed.ncbi.nlm.nih.gov/37998543/\n",
            "37996504\t\tSaliency of breast lesions in breast cancer detection using artificial intelligence\t\t\t\thttps://pubmed.ncbi.nlm.nih.gov/37996504/\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def search_pubmed(query_terms, max_articles=20):\n",
        "    # Step 1: Perform a search and get a list of PubMed IDs\n",
        "    search_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
        "    search_params = {\n",
        "        'db': 'pubmed',\n",
        "        'term': ' '.join(query_terms),\n",
        "        'retmax': max_articles  # Limit the number of articles\n",
        "    }\n",
        "\n",
        "    print(query_terms)\n",
        "\n",
        "    response = requests.get(search_url, params=search_params)\n",
        "    response_xml = response.text\n",
        "\n",
        "    # Extract PubMed IDs and filter out articles without abstracts\n",
        "    article_ids = [id for id in extract_pubmed_ids(response_xml) if has_abstract(id)]\n",
        "\n",
        "    return article_ids[:max_articles]  # Return only the specified number of article IDs\n",
        "\n",
        "def has_abstract(article_id):\n",
        "    # Check if the article has an abstract\n",
        "    fetch_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
        "    fetch_params = {\n",
        "        'db': 'pubmed',\n",
        "        'id': article_id,\n",
        "        'retmode': 'xml',\n",
        "    }\n",
        "\n",
        "    response = requests.get(fetch_url, params=fetch_params)\n",
        "    response_xml = response.text\n",
        "\n",
        "    return '<AbstractText>' in response_xml\n",
        "\n",
        "def extract_pubmed_ids(xml_response):\n",
        "    # Parse the XML response and extract PubMed IDs\n",
        "    article_ids = []\n",
        "    for line in xml_response.split('\\n'):\n",
        "        if '<Id>' in line:\n",
        "            article_ids.append(line.replace('<Id>', '').replace('</Id>', ''))\n",
        "    return article_ids\n",
        "\n",
        "def retrieve_abstract(article_id):\n",
        "    # Retrieve abstract for a given PubMed ID\n",
        "    fetch_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
        "    fetch_params = {\n",
        "        'db': 'pubmed',\n",
        "        'id': article_id,\n",
        "        'retmode': 'xml',\n",
        "    }\n",
        "\n",
        "    response = requests.get(fetch_url, params=fetch_params)\n",
        "    response_xml = response.text\n",
        "\n",
        "    # Extract abstract from the XML response\n",
        "    abstract_start = response_xml.find('<AbstractText>') + len('<AbstractText>')\n",
        "    abstract_end = response_xml.find('</AbstractText>', abstract_start)\n",
        "\n",
        "    if abstract_start != -1 and abstract_end != -1:\n",
        "        abstract = response_xml[abstract_start:abstract_end].strip()\n",
        "    else:\n",
        "        abstract = \"Abstract not available\"\n",
        "\n",
        "    return abstract\n",
        "\n",
        "def generate_openai_completion(input_text, research_question):\n",
        "    #Initialize OpenAI client\n",
        "    client = OpenAI(api_key=userdata.get('openai'))\n",
        "\n",
        "    # Response format\n",
        "    response_format_begin = \"Based on the retrieved abstracts, I think the answer to your question is:\"\n",
        "\n",
        "    # Convert input text to a more focused prompt\n",
        "    prompt = f\"Please answer this question: {research_question} using only the list of abstracts retrieved from PubMed here: {input_text}. Format your response by starting with {response_format_begin}. Be sure to answer a question directly, usually a yes or no, followed by supporting reasons you found in the abstracts.\"\n",
        "\n",
        "    try:\n",
        "        # Make a completion request to GPT-3\n",
        "        response = client.completions.create(\n",
        "            model=\"gpt-3.5-turbo-instruct\",\n",
        "            prompt=prompt,\n",
        "            max_tokens=500\n",
        "        )\n",
        "\n",
        "        # Get the generated text from the response\n",
        "        return response.choices[0].text\n",
        "\n",
        "    except Exception as e:\n",
        "        # Handle exceptions and print an error message\n",
        "        print(f\"Error: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_pubmed_info(article_id):\n",
        "    # Extract title and URL from PubMed for a given PubMed ID\n",
        "    fetch_url = f\"https://pubmed.ncbi.nlm.nih.gov/{article_id}\"\n",
        "\n",
        "    response = requests.get(fetch_url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Extract title\n",
        "    title_tag = soup.find('meta', attrs={'name': 'citation_title'})\n",
        "    title = title_tag['content'] if title_tag else \"Title not available\"\n",
        "\n",
        "    # Construct URL\n",
        "    article_url = f\"https://pubmed.ncbi.nlm.nih.gov/{article_id}/\"\n",
        "\n",
        "    return title, article_url\n",
        "\n",
        "def generate_and_display_table(article_ids):\n",
        "    print(\"\\nTable of Retrieved Articles:\")\n",
        "    print(\"PubMed ID\\tTitle\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tURL\")\n",
        "    for article_id in article_ids:\n",
        "        title, article_url = extract_pubmed_info(article_id)\n",
        "        print(f\"{article_id}\\t\\t{title}\\t\\t\\t\\t{article_url}\")\n",
        "\n",
        "def summarize_abstracts(article_ids, research_question):\n",
        "    # Retrieve abstracts and accumulate them\n",
        "    all_abstracts = \"\"\n",
        "    for article_id in article_ids:\n",
        "        abstract = retrieve_abstract(article_id)\n",
        "        # print(f\"PubMed ID: {article_id}\\nAbstract:\\n{abstract}\\n{'='*30}\")\n",
        "        if abstract is not None:\n",
        "            all_abstracts += abstract + \"\\n\\n\"\n",
        "\n",
        "    # Send all abstracts and research question to openAI to answer\n",
        "    summary = generate_openai_completion(all_abstracts, research_question)\n",
        "\n",
        "    return summary\n",
        "\n",
        "def main():\n",
        "    # Get user question\n",
        "    research_question = input(\"Enter a question: \").split()\n",
        "\n",
        "    # Get user input for search terms\n",
        "    query_terms = input(\"Enter a list of terms separated by spaces: \").split()\n",
        "\n",
        "    # Call the function to search PubMed and retrieve abstracts\n",
        "    article_ids = search_pubmed(query_terms)\n",
        "\n",
        "    if article_ids:\n",
        "        # Call the function to summarize the retrieved abstracts\n",
        "        summary = summarize_abstracts(article_ids, research_question)\n",
        "\n",
        "        # Print or handle the generated summary as needed\n",
        "        print(f\"Answer:\\n{summary}\")\n",
        "    else:\n",
        "        print(\"No articles found.\")\n",
        "\n",
        "    # Generate the list of references that were used\n",
        "    generate_and_display_table(article_ids)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}