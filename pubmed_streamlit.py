# -*- coding: utf-8 -*-
"""pubmed streamlit.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15LnFY710nEwW5bXcYEJhBzePmaDDFuIZ
"""

import streamlit as st
import requests
import os
from openai import OpenAI

def generate_keywords(research_question):
    #Initialize OpenAI client
    openai_api_key = os.getenv("openai_api_key")
    client = OpenAI(api_key=openai_api_key)

    # Prompt
    prompt = f"Please generate an ideal set of keywords that could be used in a pubmed search based on this research question:{research_question}\n\n"

    try:
        # Make a completion request to GPT-3
        response = client.completions.create(
            model="text-davinci-003",
            prompt=prompt,
            max_tokens=1000 # Adjust the max tokens as needed
        )

        # Get the generated text from the response
        return response.choices[0].text

    except Exception as e:
        # Handle exceptions and print an error message
        print(f"Error: {e}")
        return None

def search_pubmed(query_terms, max_articles=25):
    # Step 1: Perform a search and get a list of PubMed IDs
    search_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi"
    search_params = {
        'db': 'pubmed',
        'term': ' '.join(query_terms),
        'retmax': max_articles  # Limit the number of articles
    }

    response = requests.get(search_url, params=search_params)
    response_xml = response.text

    # Extract PubMed IDs and filter out articles without abstracts
    article_ids = [id for id in extract_pubmed_ids(response_xml) if has_abstract(id)]

    return article_ids[:max_articles]  # Return only the specified number of article IDs

def has_abstract(article_id):
    # Check if the article has an abstract
    fetch_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi"
    fetch_params = {
        'db': 'pubmed',
        'id': article_id,
        'retmode': 'xml',
    }

    response = requests.get(fetch_url, params=fetch_params)
    response_xml = response.text

    return '<AbstractText>' in response_xml

def extract_pubmed_ids(xml_response):
    # Parse the XML response and extract PubMed IDs
    article_ids = []
    for line in xml_response.split('\n'):
        if '<Id>' in line:
            article_ids.append(line.replace('<Id>', '').replace('</Id>', ''))
    return article_ids

def retrieve_abstract(article_id):
    # Retrieve abstract for a given PubMed ID
    fetch_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi"
    fetch_params = {
        'db': 'pubmed',
        'id': article_id,
        'retmode': 'xml',
    }

    response = requests.get(fetch_url, params=fetch_params)
    response_xml = response.text

    # Extract abstract from the XML response
    abstract_start = response_xml.find('<AbstractText>') + len('<AbstractText>')
    abstract_end = response_xml.find('</AbstractText>', abstract_start)

    if abstract_start != -1 and abstract_end != -1:
        abstract = response_xml[abstract_start:abstract_end].strip()
    else:
        abstract = "Abstract not available"

    return abstract

def generate_openai_completion(input_text, research_question):
    #Initialize OpenAI client
    openai_api_key = os.getenv("openai_api_key")
    client = OpenAI(api_key=openai_api_key)

    # Convert input text to a more focused prompt
    prompt = f"Please use the retrieved abstracts from pubmed and answer the research question posed: {research_question}\n\n{input_text}"

    try:
        # Make a completion request to GPT-3
        response = client.completions.create(
            model="text-davinci-003",
            prompt=prompt,
            max_tokens=1000 # Adjust the max tokens as needed
        )

        # Get the generated text from the response
        return response.choices[0].text

    except Exception as e:
        # Handle exceptions and print an error message
        print(f"Error: {e}")
        return None

def summarize_abstracts(article_ids, research_question):
    # Retrieve abstracts and accumulate them
    article_data = {"PubMed ID": [], "Title": []}

    for article_id in article_ids:
        abstract = retrieve_abstract(article_id)
        title = retrieve_article_title(article_id)

        article_data["PubMed ID"].append(article_id)
        article_data["Title"].append(title)

    # Generate a summary for all abstracts, including the research question
    all_abstracts = "\n\n".join([retrieve_abstract(article_id) for article_id in article_ids if retrieve_abstract(article_id) is not None])
    summary = generate_openai_completion(all_abstracts, research_question)

    return summary, article_data

def retrieve_article_title(article_id):
    # Retrieve article title for a given PubMed ID
    fetch_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi"
    fetch_params = {
        'db': 'pubmed',
        'id': article_id,
        'retmode': 'xml',
    }

    response = requests.get(fetch_url, params=fetch_params)
    response_xml = response.text

    # Extract article title from the XML response
    title_start = response_xml.find('<ArticleTitle>') + len('<ArticleTitle>')
    title_end = response_xml.find('</ArticleTitle>', title_start)

    if title_start != -1 and title_end != -1:
        title = response_xml[title_start:title_end].strip()
    else:
        title = "Title not available"

    return title

def main():
    st.title("PubMed Abstract Summarizer")

    # Get user's research question
    research_question = st.text_input("Enter your research question:")

    # Use the research question as a prompt to generate suggested keywords
    suggested_keywords = generate_keywords(research_question)

    if suggested_keywords:
        st.subheader("Suggested Keywords:")
        st.write(suggested_keywords)

        if st.button("Search and Summarize"):
            # Display loading message
            with st.spinner("Searching and summarizing..."):
                article_ids = search_pubmed(suggested_keywords)

                if article_ids:
                    summary, article_data = summarize_abstracts(article_ids, research_question)
                    st.subheader("Summary of all abstracts:")
                    st.write(summary)

                    st.subheader("List of PubMed Articles:")
                    st.table(article_data)
                else:
                    st.warning("No articles found.")

if __name__ == "__main__":
    main()