# -*- coding: utf-8 -*-
"""pubmed streamlit.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15LnFY710nEwW5bXcYEJhBzePmaDDFuIZ
"""

import streamlit as st
import requests
import os
from openai import OpenAI

def search_pubmed(query_terms, max_articles=25):
    # Step 1: Perform a search and get a list of PubMed IDs
    search_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi"
    search_params = {
        'db': 'pubmed',
        'term': ' '.join(query_terms),
        'retmax': max_articles  # Limit the number of articles
    }

    response = requests.get(search_url, params=search_params)
    response_xml = response.text

    # Extract PubMed IDs and filter out articles without abstracts
    article_ids = [id for id in extract_pubmed_ids(response_xml) if has_abstract(id)]

    return article_ids[:max_articles]  # Return only the specified number of article IDs

def has_abstract(article_id):
    # Check if the article has an abstract
    fetch_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi"
    fetch_params = {
        'db': 'pubmed',
        'id': article_id,
        'retmode': 'xml',
    }

    response = requests.get(fetch_url, params=fetch_params)
    response_xml = response.text

    return '<AbstractText>' in response_xml

def extract_pubmed_ids(xml_response):
    # Parse the XML response and extract PubMed IDs
    article_ids = []
    for line in xml_response.split('\n'):
        if '<Id>' in line:
            article_ids.append(line.replace('<Id>', '').replace('</Id>', ''))
    return article_ids

def retrieve_abstract(article_id):
    # Retrieve abstract for a given PubMed ID
    fetch_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi"
    fetch_params = {
        'db': 'pubmed',
        'id': article_id,
        'retmode': 'xml',
    }

    response = requests.get(fetch_url, params=fetch_params)
    response_xml = response.text

    # Extract abstract from the XML response
    abstract_start = response_xml.find('<AbstractText>') + len('<AbstractText>')
    abstract_end = response_xml.find('</AbstractText>', abstract_start)

    if abstract_start != -1 and abstract_end != -1:
        abstract = response_xml[abstract_start:abstract_end].strip()
    else:
        abstract = "Abstract not available"

    return abstract

def generate_openai_completion(input_text, research_question):
    #Initialize OpenAI client
    openai_api_key = os.getenv("openai_api_key")
    client = OpenAI(api_key=openai_api_key)

    # Convert input text to a more focused prompt
    prompt = f"Please use the retrieved abstracts from pubmed and answer the research question posed: {research_question}\n\n{input_text}"

    try:
        # Make a completion request to GPT-3
        response = client.completions.create(
            model="text-davinci-003",
            prompt=prompt,
            max_tokens=1000 # Adjust the max tokens as needed
        )

        # Get the generated text from the response
        return response.choices[0].text

    except Exception as e:
        # Handle exceptions and print an error message
        print(f"Error: {e}")
        return None

def summarize_abstracts(article_ids, research_question):
    # Retrieve abstracts and accumulate them
    all_abstracts = ""
    article_data = {"PubMed ID": [], "Title": []}

    for article_id in article_ids:
        abstract = retrieve_abstract(article_id)
        title = retrieve_article_title(article_id)

        article_data["PubMed ID"].append(article_id)
        article_data["Title"].append(title)

        st.write(f"PubMed ID: {article_id}\nTitle: {title}\nAbstract:\n{abstract}\n{'=' * 30}")

        if abstract is not None:
            all_abstracts += abstract + "\n\n"

    # Generate a summary for all abstracts, including the research question
    summary = generate_openai_completion(all_abstracts, research_question)

    return summary, article_data

def retrieve_article_title(article_id):
    # Retrieve article title for a given PubMed ID
    fetch_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi"
    fetch_params = {
        'db': 'pubmed',
        'id': article_id,
        'retmode': 'xml',
    }

    response = requests.get(fetch_url, params=fetch_params)
    response_xml = response.text

    # Extract article title from the XML response
    title_start = response_xml.find('<ArticleTitle>') + len('<ArticleTitle>')
    title_end = response_xml.find('</ArticleTitle>', title_start)

    if title_start != -1 and title_end != -1:
        title = response_xml[title_start:title_end].strip()
    else:
        title = "Title not available"

    return title

def main():
    st.title("PubMed Abstract Summarizer")
    research_question = st.text_input("Enter your research question:")
    query_terms = st.text_input("Enter a list of terms separated by spaces:")

    if st.button("Search and Summarize"):
        article_ids = search_pubmed(query_terms.split())

        if article_ids:
            summary, article_data = summarize_abstracts(article_ids, research_question)
            st.subheader("Summary of all abstracts:")
            st.write(summary)

            st.subheader("List of PubMed Articles:")
            st.table(article_data)
        else:
            st.warning("No articles found.")

if __name__ == "__main__":
    main()